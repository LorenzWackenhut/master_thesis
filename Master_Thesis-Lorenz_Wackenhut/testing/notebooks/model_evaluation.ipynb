{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "from pyspark.ml.classification import RandomForestClassificationModel\n",
                "from pyspark.ml.classification import GBTClassificationModel\n",
                "from pyspark.ml.tuning import CrossValidatorModel\n",
                "from pyspark.ml.pipeline import PipelineModel\n",
                "from pyspark.sql.types import *\n",
                "\n",
                "\n",
                "class Loader():\n",
                "    \"\"\"\n",
                "    Includes all methods to read or write from the ADLS storage\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, spark_session, adls_account_name='REDACTED'):\n",
                "        self.spark_session = spark_session\n",
                "        self.spark_context = spark_session.sparkContext\n",
                "        self.adls_account_name = adls_account_name\n",
                "\n",
                "    def readDataframeFromAdls(self, table_path, container_name='data'):\n",
                "        \"\"\"\n",
                "        Reads a dataframe in parquet format from ADLS\n",
                "\n",
                "        Paramters\n",
                "        -----\n",
                "        table_path: str, required\n",
                "            absolute path to table in ADLS container\n",
                "        container_name: str, optional\n",
                "            name of the container in ADLS\n",
                "\n",
                "        Returns\n",
                "        -----\n",
                "        Spark dataframe\n",
                "        \"\"\"\n",
                "        return (\n",
                "            self\n",
                "            .spark_session\n",
                "            .read\n",
                "            .parquet(\n",
                "                f'abfss://{container_name}@{self.adls_account_name}.dfs.core.windows.net{table_path}')\n",
                "        )\n",
                "\n",
                "    def readModelFromAdls(self, model_path, model_class, container_name='data'):\n",
                "        \"\"\"\n",
                "        Reads a RandomForestClassificationModel from ADLS\n",
                "\n",
                "        Paramters\n",
                "        -----\n",
                "        model_class; str, required\n",
                "            class name of the model e.g. RandomForestClassificationModel\n",
                "        model_path: str, required\n",
                "            absolute path to model in ADLS container\n",
                "        container_name: str, optional\n",
                "            name of the container in ADLS\n",
                "\n",
                "        Returns\n",
                "        -----\n",
                "        PipelineModel\n",
                "        \"\"\"\n",
                "        model = globals()[model_class]\n",
                "        return (\n",
                "            model\n",
                "            .load(\n",
                "                f\"abfss://{container_name}@{self.adls_account_name}.dfs.core.windows.net{model_path}\"\n",
                "            )\n",
                "        )"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import pyspark\n",
                "from pyspark.sql.types import *\n",
                "from pyspark import SparkContext\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql import SQLContext\n",
                "import pyspark.sql.functions as F\n",
                "import pyspark.ml.feature as ML\n",
                "from pyspark.ml import Pipeline\n",
                "from pyspark.ml.classification import RandomForestClassifier\n",
                "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
                "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
                "from sklearn.metrics import classification_report, confusion_matrix, auc, roc_curve\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "account_name = \"REDACTED\"\n",
                "account_key = \"5S/FS3ZJpxmiu8CzAVWCyKnFhK0tymvu3thvsvvUqUvkaP2w75skbLHMkzHpycSB35wo4Tf26v1rjiYTEmV9Gw==\"\n",
                "\n",
                "spark = (\n",
                "    SparkSession\n",
                "        .builder\n",
                "        .getOrCreate()\n",
                ")\n",
                "\n",
                "sc = spark.sparkContext"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "Py4JJavaError",
                    "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: Configuration property REDACTED.dfs.core.windows.net not found.\n\tat org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:372)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:1133)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:174)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:110)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.spark.util.Utils$.getHadoopFileSystem(Utils.scala:1915)\n\tat org.apache.spark.deploy.history.EventLogFileWriter.<init>(EventLogFileWriters.scala:60)\n\tat org.apache.spark.deploy.history.SingleEventLogFileWriter.<init>(EventLogFileWriters.scala:213)\n\tat org.apache.spark.deploy.history.EventLogFileWriter$.apply(EventLogFileWriters.scala:181)\n\tat org.apache.spark.scheduler.EventLoggingListener.<init>(EventLoggingListener.scala:66)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:603)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-2-31abc7be8f81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m spark = (\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0m\u001b[1;32m    147\u001b[0m                           conf, jsc, profiler_cls)\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1569\u001b[0m             answer, self._gateway_client, None, self._fqn)\n\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: Configuration property REDACTED.dfs.core.windows.net not found.\n\tat org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:372)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:1133)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:174)\n\tat org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:110)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.spark.util.Utils$.getHadoopFileSystem(Utils.scala:1915)\n\tat org.apache.spark.deploy.history.EventLogFileWriter.<init>(EventLogFileWriters.scala:60)\n\tat org.apache.spark.deploy.history.SingleEventLogFileWriter.<init>(EventLogFileWriters.scala:213)\n\tat org.apache.spark.deploy.history.EventLogFileWriter$.apply(EventLogFileWriters.scala:181)\n\tat org.apache.spark.scheduler.EventLoggingListener.<init>(EventLoggingListener.scala:66)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:603)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "loader = Loader(spark, account_name)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "df_predict = loader.readDataframeFromAdls('/transformation/df_transform_training')\n",
                "gbt_hyper = loader.readModelFromAdls('/model/gbt_hyper', 'GBTClassificationModel')\n",
                "rfc_hyper = loader.readModelFromAdls('/model/rfc_hyper', 'RandomForestClassificationModel')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "df_predicted_gbt = gbt_hyper.transform(df_predict)\n",
                "df_predicted_rfc = rfc_hyper.transform(df_predict)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def getPerformanceMetrics(df):\n",
                "    metrics_dictionary = {}\n",
                "    evaluator_multi = MulticlassClassificationEvaluator(\n",
                "        labelCol='Y', predictionCol=\"prediction\")\n",
                "    evaluator_binary = BinaryClassificationEvaluator(\n",
                "        labelCol='Y', rawPredictionCol=\"prediction\", metricName='areaUnderROC')\n",
                "\n",
                "    # Get metrics\n",
                "    metrics_dictionary['acc'] = evaluator_multi.evaluate(\n",
                "        df, {evaluator_multi.metricName: \"accuracy\"})\n",
                "    metrics_dictionary['f1'] = evaluator_multi.evaluate(\n",
                "        df, {evaluator_multi.metricName: \"f1\"})\n",
                "    metrics_dictionary['weighted_precision'] = evaluator_multi.evaluate(\n",
                "        df, {evaluator_multi.metricName: \"weightedPrecision\"})\n",
                "    metrics_dictionary['weighted_recall'] = evaluator_multi.evaluate(\n",
                "        df, {evaluator_multi.metricName: \"weightedRecall\"})\n",
                "    metrics_dictionary['auc'] = evaluator_binary.evaluate(df)\n",
                "\n",
                "    print(f\"Accuracy: {metrics_dictionary['acc']}\")\n",
                "    print(f\"F1: {metrics_dictionary['f1']}\")\n",
                "    print(\n",
                "        f\"Weighted Precision: {metrics_dictionary['weighted_precision']}\")\n",
                "    print(f\"Weighted Recall: {metrics_dictionary['weighted_recall']}\")\n",
                "    print(f\"AUC: {metrics_dictionary['auc']}\")\n",
                "\n",
                "    return metrics_dictionary"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def printConfusionMatrix(df):\n",
                "    y_true = df.select(['Y']).collect()\n",
                "    y_pred = df.select(['prediction']).collect()\n",
                "\n",
                "    print(classification_report(y_true, y_pred))\n",
                "    print(confusion_matrix(y_true, y_pred))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "class CurveMetrics(BinaryClassificationMetrics):\n",
                "    def __init__(self, *args):\n",
                "        super(CurveMetrics, self).__init__(*args)\n",
                "\n",
                "    def _to_list(self, rdd):\n",
                "        points = []\n",
                "        # Note this collect could be inefficient for large datasets \n",
                "        # considering there may be one probability per datapoint (at most)\n",
                "        # The Scala version takes a numBins parameter, \n",
                "        # but it doesn't seem possible to pass this from Python to Java\n",
                "        for row in rdd.collect():\n",
                "            # Results are returned as type scala.Tuple2, \n",
                "            # which doesn't appear to have a py4j mapping\n",
                "            points += [(float(row._1()), float(row._2()))]\n",
                "        return points\n",
                "\n",
                "    def get_curve(self, method):\n",
                "        rdd = getattr(self._java_model, method)().toJavaRDD()\n",
                "        return self._to_list(rdd)\n",
                "\n",
                "def plotROC(df):\n",
                "    preds = df.select('Y','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['Y'])))\n",
                "    points = CurveMetrics(preds).get_curve('roc')\n",
                "\n",
                "    plt.figure()\n",
                "    x_val = [x[0] for x in points]\n",
                "    y_val = [x[1] for x in points]\n",
                "    plt.title('title')\n",
                "    plt.xlabel('xlabel')\n",
                "    plt.ylabel('ylabel')\n",
                "    plt.plot(x_val, y_val)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def ExtractFeatureImp(model, df, featuresCol='X'):\n",
                "    featureImp = model.featureImportances\n",
                "    list_extract = []\n",
                "    for i in df.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
                "        list_extract = list_extract + df.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
                "    varlist = pd.DataFrame(list_extract)\n",
                "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
                "    return(varlist.sort_values('score', ascending = False))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}